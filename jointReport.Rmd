---
title: "COD Reproducibility Report"
output:
  html_document:
    toc: true
    float: true
---

#### Article ID: DFDwT
#### Pilot: Camilla Griffiths
#### Co-pilot: Gustav Nilsonne
#### Start date: 03/12/2017
#### End date: 03/17/2017 

-------

#### Methods summary: 

After excluding trials based on stated exclusion criteria (i.e. extreme reproduction intervals: >3SD from individual & grand mean), the analyses will include a 2X2 ANOVA testing the main and interaction effects of 'presence of agency' and 'physical effort' on the accuracy of interval replication between two events. Participants in this within-subjects experiment were either asked to press a button and then listen for a tone (agency) or to listen to an initial tone then a second tone (no agency) while holding a resistance band in their right hand that was either high or low resistance/physical effort. 

I will also set contrasts to test the effects of each individual condition on the outcome of interval reproduction errors. These contrasts will entail comparing low vs. high physical effort and comparing agency vs. no agency conditions. 

------

#### Target outcomes: 


"Trials with extreme reproduction errors were removed (0.48% of trials). Mean reproduction errors for each participant in each condition (see Fig. 3) were analysed using a 2 (“Presence of Agency”) × 2 (“Physical Effort”) ANOVA, revealing significant main effects of both of these factors: F(1, 34) = 54.54, p < 0.001, ηp2 = 0.62, and F(1, 34) = 14.43, p = 0.001, ηp2 = 0.3, respectively. These main effects were due to larger reproduction errors - more underestimation - in the “Agency” condition than the “No Agency” condition, (M = −322 ms, SD = 236 and M = −56 ms, SD = 265, respectively). This replicates the basic temporal binding effect ( Buehner and Humphreys, 2009; Haggard et al., 2002; Humphreys and Buehner, 2010 ; Poonian and Cunnington, 2013). There were larger reproduction errors in the “Low” effort compared with “High” effort conditions, M = -208 ms, SD = 230 and M = −170 ms, SD = 228, respectively. However, the critical interaction was non-significant, F(1, 34) = 2.12, p = 0.154, ηp2 = 0.06.

Although the interaction – critical to our research question – was non-significant, we nevertheless performed two planned contrasts to investigate the nature of effects across “Presence of Agency” and “Physical Effort”. In the “Agency” condition, reproduction errors were significantly larger under “Low” than “High” effort, t(34) = 3.46, p = 0.001, dz = 0.589, while this effect was not significant in the “No Agency” condition, t(34) = 1.59, p = 0.122, dz = 0.27. Therefore, due to a non-significant interaction, the overall results do not support our hypothesis that reproduction errors in agency tasks would be reduced under greater physical effort." (from Howard et al, 2016 p.117)"

------


```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

## Step 1: Load packages

```{r}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CODreports) # custom report functions
library(ez) # for anova
```

## Step 2: Load data

I could not load the data as is into R because it was highly formatted with multiple tabs, headers and subheaders in Excel so I made the following changes in Excel in order to be able to import into R: 
1. Copied just Experiment 1 data (only data of interest here) into a new Excel workbook
2. Saved the new workbook as data_ManualClean
3. Changed 'Participant ID' to PID 
4. Changed condition variable labels so that it includes information about both conditions (to be able to get rid of double headers) 
  - Agency | low effort --> A_lowE
  - Agency | high effort --> A_highE
  - No Agency | low effort --> NA_lowE
  - No Agency | high effort --> NA_highE
5. Changed condition variable names to reflect order of trials -
  - Agency |1st half | low effort --> 1A_lowE
  - Agency |1st half | high effort --> 1A_highE
  - Agency |2nd half | low effort --> 2A_lowE
  - Agency |2nd half | high effort --> 2A_highE
  - No Agency |1st half | low effort --> 1NA_lowE
  - No Agency |1st half | high effort --> 1NA_highE
  - No Agency |2nd half | low effort --> 2NA_lowE
  - No Agency |2nd half | high effort --> 2NA_highE
6. Changed variable names to reflect pre- and post-task effort ratings -  again to be able to delete multiple headers and make variable names descriptive 
  - low effort pre --> preE_low
  - low effort post --> postE_low
  - high effort pre --> preE_high
  - high effort post --> preE_high
7. Changed main DV variable name from 'Reproduction Error Differential' to 'RepError_D' 
8. Changed 'Error Differential' variable name to 'Error_D'

Co-pilot's note: Have not attempted to reproduce this step.
  
```{r}
expt1 <- read_csv("data/data_ManualClean.csv", 
     col_types = cols(`Gender (1=female)` = col_factor(levels = c("Female", "Male"))))
```

## Step 3: Tidy data

To reproduce analyses in the specified 2.2 results section, I will only be looking at the interval reproduction data in both conditions (effort & agency) - this means excluding the trial order variables as well as the manipulation check variables in this tidy dataframe. 

```{r}
expt1.tidy= expt1 %>%
  select(PID, A_lowE:NA_highE) %>%
  gather(title, rating, A_lowE:NA_highE) %>%
  separate(title, c("agency", "effort"), sep="_")
```

## Step 4: Run analysis

### Pre-processing

As they did in their analysis, we'll remove trials that are >3SD from a participants' mean interval reproductions and to remove trials that are >3SD from the grand mean. 

```{r}
#filtering Ps trials that are >3SD from grand mean
expt1.tidy= expt1.tidy %>%
  mutate(zscore=scale(rating)) %>%
  filter(zscore<3)
  
#filtering Ps trials that are >3SD from their individual means
expt1.tidy= expt1.tidy %>%
  group_by(PID)%>%
  mutate(indiv_zscore=scale(rating)) %>%
  filter(indiv_zscore<3)

summary(expt1.tidy$zscore)
```

It looks like they might have done their exclusions prior to uploading their data - no trials appeared to be anywhere close to 3SD from the grand mean or individual means - the maximum zscore was 2.5, which is what leads me to believe they performed their exclusions before posting the data. 

Therefore, there were no trials excluded in this replication analysis. 

### Descriptive statistics

```{r}
#mean reproduction errors by agency condition
expt1.tidy %>%
  mutate(agency=as.factor(agency))%>%
  mutate(effort=as.factor(effort)) %>%
  group_by(agency) %>%
  summarise(mean_agency= mean(rating), SD_agency=sd(rating))
#mean reproduction errors by effort condition 
expt1.tidy %>%
  group_by(effort) %>%
  summarise(mean_effort= mean(rating), SD_effort=sd(rating))

#Descriptive Statistic Errors in Standard Deviation results!
compareValues(reportedValue = 236, obtainedValue = 239)
compareValues(reportedValue = 265, obtainedValue = 266)
compareValues(reportedValue = 230, obtainedValue = 288)
compareValues(reportedValue = 228, obtainedValue = 283)
```

There were a number of numerical errors in the descriptive statistics reported in the paper, namely in the reported standard deviation values for the mean agency and effort condition values. Below is a summary of those differences. 

1. Agency condition: M= -322ms, SD=239 (3pt difference from reported SD: 236)
2. No Agency condition: M=-56ms, SD=266 (1pt difference from reported SD: 265)

3. Low Effort condition: M=-208ms, SD=288 (58pt difference from reported SD:230)
4. High Effort condition: M=-170ms, SD=283 (55pt difference from reported SD:228)

### Inferential statistics

```{r}
#2x2 anova to test main effects and interaction of agency & effort conditions 
expt1.tidy$agency=as.factor(expt1.tidy$agency)
expt1.tidy$effort=as.factor(expt1.tidy$effort)
expt1.tidy$PID=as.factor(expt1.tidy$PID)

res1=ezANOVA(dv= .(rating), wid= .(PID), within= .(agency, effort), detailed=TRUE, data=expt1.tidy)
res1

unlisted <- unlist(res1)

aov_stats <- tibble(effect = c("agency", "effort", "agency:effort"),
  F = c(as.double(unlisted["ANOVA.F2"]), as.double(unlisted["ANOVA.F3"]), as.double(unlisted["ANOVA.F4"])), 
  p = c(as.double(unlisted["ANOVA.p2"]), as.double(unlisted["ANOVA.p3"]), as.double(unlisted["ANOVA.p4"])),
  SSn = c(as.double(unlisted["ANOVA.SSn2"]), as.double(unlisted["ANOVA.SSn3"]), as.double(unlisted["ANOVA.SSn4"])),
  SSd = c(as.double(unlisted["ANOVA.SSd2"]), as.double(unlisted["ANOVA.SSd3"]), as.double(unlisted["ANOVA.SSd4"]))) %>% 
  mutate(partial_eta_squared = SSn / (SSn + SSd)) %>% 
  select(-SSn, -SSd)


aov_stats

#Compare stats for main effects and interaction 
compareValues(reportedValue = 54.54, obtainedValue = aov_stats$F[1]) #agency main effect F statistic
compareValues(reportedValue = 0.62, obtainedValue = 0.62) #agency main effect partial eta squared
compareValues(reportedValue = 14.43, obtainedValue = 14.43) #effort main effect F statistic
compareValues(reportedValue = 0.3, obtainedValue = 0.3) #effort main effect partial eta squared
compareValues(reportedValue = 2.12, obtainedValue = 2.12) #effort*agency interaction F statistic
compareValues(reportedValue = 0.06, obtainedValue = 0.06) #effort*agency interaction partial eta squared 

```

For their main analysis to test their hypothesis, they ran a 2X2 anova testing the main effects and interaction of effort and agency conditions on the interval reproduction errors outcome variable. Replication of these anayleses yielded the same exact statistics reported in section 2.2 of their results section for experiment 1. There were not any minor numerical errors or otherwise in these findings. 


From the paper: 
> "Although the interaction – critical to our research question –
was non-significant, we nevertheless performed two planned contrasts
to investigate the nature of effects across ‘‘Presence of
Agency” and ‘‘Physical Effort”. In the ‘‘Agency” condition, reproduction
errors were significantly larger under ‘‘Low” than ‘‘High”
effort, t(34) = 3.46, p = 0.001, dz = 0.589, while this effect was not
significant in the ‘‘No Agency” condition, t(34) = 1.59, p = 0.122,
dz = 0.27."

```{r planned contrasts}
#library(lmerTest)
#summary(lm(A_lowE - A_highE ~1, data=expt1))
#summary(lm(NA_lowE - NA_highE ~1, data=expt1))

#compareValues(reportedValue = 3.46, obtainedValue = 3.46) #Within agency cells effort effects -- t-statistic
#compareValues(reportedValue = 0.001, obtainedValue = 0.001, isP = T) #Within agency cells effort effects -- p-value
#compareValues(reportedValue = 1.59, obtainedValue = -1.59) #Within no agency cells effort effects -- t-statistic
#compareValues(reportedValue = 0.122, obtainedValue = 0.122, isP = T) #Within no agency cells effort effects -- p-value

# Alternative approach by GN on advice of MF
ttest1 <- t.test(expt1$A_lowE, expt1$A_highE, paired = T)
ttest2 <- t.test(expt1$NA_lowE, expt1$NA_highE, paired = T)

compareValues(reportedValue = -1.59, obtainedValue = ttest2$statistic) 

# Calculate dz
dz1 <- mean(expt1$A_lowE - expt1$A_highE) / sd(expt1$A_lowE - expt1$A_highE)
dz2 <- mean(expt1$NA_lowE - expt1$NA_highE) / sd(expt1$NA_lowE - expt1$NA_highE)

compareValues(reportedValue = 0.589, obtainedValue = abs(dz1)) 

```
In order to replicate the planned contrasts that they performed, I ran a regression testing the difference in interval reproduction ratings between Effort levels within each agency condition, which produced the exact same statistics reported in the paper, with one big exception. It appears that the authors reported the t-statistic for the 'No Agency' condition contrast with the wrong sign, reporting a positive value when the value I obtained was the same number, just negative. 

## Step 5: Conclusion

```{r}

codReport(Report_Type = 'pilot',
          Article_ID = 'DFDwT', 
          Insufficient_Information_Errors = 0,
          Decision_Errors = 0, 
          Major_Numerical_Errors = 3, 
          Minor_Numerical_Errors = 5)
```


This reproducibility check appears to be a success in terms of the main findings and statistics being reproduced, but there were a few significant numerical errors that might classify the reproduction a failure. Namely, the standard deviation numbers for the mean interval reproduction errors for effort and agency conditions were off in the order of the hundreds. Moreover, there was a t-statistic reported with the opposite sign (positive instead of negative) than what was yielded in this check. Otherwise, I was able to reproduce the main 2x2 anova analyses, which yielded two significant main effects of agency and effort conditions but no significant interaction effect. The failure to yield a significant interaction effect meant that the authors were unable to support their hypothesis that reproduction errors in agency tasks would be reduced under greater physical effort. 

Comment by GN: Added checks of dz values, one of which turned out a minor numerical error. Also changed from linear models to standard t-tests, which caused another minor numerical error to turn up. These errors are small and could well be explained by differences in software packages or such.


[This function will output information about the package versions used in this report:]

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
